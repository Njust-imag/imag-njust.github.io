<!DOCTYPE html><html lang="en"><head><meta name="generator" content="React Static"/><meta charSet="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5, shrink-to-fit=no"/><link rel="preload" as="script" href="/templates/styles.0fac0af2.js"/><link rel="preload" as="script" href="/templates/vendors~main.f9fbf9ef.js"/><link rel="preload" as="script" href="/main.a7a8ff5b.js"/><link rel="preload" as="style" href="/styles.82b2ee7c.css"/><link rel="stylesheet" href="/styles.82b2ee7c.css"/></head><body><div id="root"><div style="outline:none" tabindex="-1"><div class="App"><div class="jss2"><div class="jss3"><div class="jss4"><div class="content"><div><h3 class="MuiTypography-root MuiTypography-h3 MuiTypography-alignCenter" style="color:#279DCC;height:80%">南京理工大学智能媒体分析实验室</h3></div><div><p class="MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom" style="color:#666666;margin-bottom:25px">Intelligent Media Analysis Group (IMAG), School of Computer Science, Nanjing University of Science and Technology</p></div></div></div></div><div class="jss6"><header class="MuiPaper-root MuiAppBar-root MuiAppBar-positionStatic MuiAppBar-colorPrimary jss9 MuiPaper-elevation4"><div class="MuiToolbar-root MuiToolbar-regular MuiToolbar-gutters"><div class="jss11"><div class="jss13"><div style="background-color:white"><a style="color:#57247b;padding-left:20px;padding-right:20px;line-height:64px" href="/">HOME</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px" href="/people">PEOPLE</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px" href="/news">NEWS</a></div><div><button class="MuiButtonBase-root MuiButton-root MuiButton-text" tabindex="0" type="button" style="top:-2px;font-size:16px;font-weight:bold;padding-left:20px;padding-right:20px;line-height:46px;background-color:;color:white" aria-haspopup="true"><span class="MuiButton-label">PUBLICATION</span></button></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px" href="/grants">GRANTS</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px" href="/resource">RESOURCE</a></div><div style="background-color:"><a style="color:white;padding-left:20px;padding-right:20px;line-height:64px" href="/contact">CONTACT</a></div></div></div></div></header></div><div class="jss5"><div style="outline:none" tabindex="-1"><div class="jss34"><div class="jss36"><h3 class="MuiTypography-root MuiTypography-h3 MuiTypography-alignCenter" style="padding-top:50px;padding-bottom:10px">研究室的一篇跨媒体检索论文被TCSVT接收</h3><div><p>研究室的一篇跨媒体检索论文被TCSVT接收为Regular论文: Yunbo Wang and Yuxin Peng, "MARS: Learning Modality-Agnostic Representation for Scalable Cross-media Retrieval", IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2021。祝贺王运波博士！
  现有方法在训练时受限于成对的跨媒体数据，当新增一种媒体类型时，需要再次训练现有媒体类型的数据，削弱了跨媒体检索的灵活性。针对上述问题，提出了一种媒体类型无关的表示学习方法，支持每种媒体数据独立学习判别性表示，进而实现跨媒体检索。将标签信息视为一种特殊的模态，引入标签解析模块得到标签语义表示以关联不同媒体数据； 同时，构建特定媒体的表示模块获取其语义共享表示。当新增媒体类型时，以已经学习到的标签语义表示作为特权表示来引导新增媒体数据的训练。此外，一个统一的分类器被用于不同媒体数据的表示模块，促进不同媒体数据共享表示的语义对齐，提高了跨媒体检索的有效性和灵活性。</p></div></div></div></div></div></div></div><div class="jss7"><div class="jss8"><div><p class="MuiTypography-root MuiTypography-body1" style="color:#ffffff">Copyright © 2022 南京理工大学智能媒体分析实验室. All rights reserved.</p></div><div><p class="MuiTypography-root MuiTypography-body1 MuiTypography-gutterBottom" style="color:#ffffff">Theme: ColorMag by ThemeGrill. Powered by WordPress.</p></div></div></div></div></div><script type="text/javascript">window.__routeInfo = JSON.parse("{\"template\":\"react_static_root__/src/containers/newsPage\",\"sharedHashesByProp\":{},\"data\":{\"newItem\":{\"title\":\"\u7814\u7A76\u5BA4\u7684\u4E00\u7BC7\u8DE8\u5A92\u4F53\u68C0\u7D22\u8BBA\u6587\u88ABTCSVT\u63A5\u6536\",\"publisher\":\"\",\"content\":\"<p>\u7814\u7A76\u5BA4\u7684\u4E00\u7BC7\u8DE8\u5A92\u4F53\u68C0\u7D22\u8BBA\u6587\u88ABTCSVT\u63A5\u6536\u4E3ARegular\u8BBA\u6587: Yunbo Wang and Yuxin Peng, \\\"MARS: Learning Modality-Agnostic Representation for Scalable Cross-media Retrieval\\\", IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2021\u3002\u795D\u8D3A\u738B\u8FD0\u6CE2\u535A\u58EB\uFF01\\n\u2003\u2003\u73B0\u6709\u65B9\u6CD5\u5728\u8BAD\u7EC3\u65F6\u53D7\u9650\u4E8E\u6210\u5BF9\u7684\u8DE8\u5A92\u4F53\u6570\u636E\uFF0C\u5F53\u65B0\u589E\u4E00\u79CD\u5A92\u4F53\u7C7B\u578B\u65F6\uFF0C\u9700\u8981\u518D\u6B21\u8BAD\u7EC3\u73B0\u6709\u5A92\u4F53\u7C7B\u578B\u7684\u6570\u636E\uFF0C\u524A\u5F31\u4E86\u8DE8\u5A92\u4F53\u68C0\u7D22\u7684\u7075\u6D3B\u6027\u3002\u9488\u5BF9\u4E0A\u8FF0\u95EE\u9898\uFF0C\u63D0\u51FA\u4E86\u4E00\u79CD\u5A92\u4F53\u7C7B\u578B\u65E0\u5173\u7684\u8868\u793A\u5B66\u4E60\u65B9\u6CD5\uFF0C\u652F\u6301\u6BCF\u79CD\u5A92\u4F53\u6570\u636E\u72EC\u7ACB\u5B66\u4E60\u5224\u522B\u6027\u8868\u793A\uFF0C\u8FDB\u800C\u5B9E\u73B0\u8DE8\u5A92\u4F53\u68C0\u7D22\u3002\u5C06\u6807\u7B7E\u4FE1\u606F\u89C6\u4E3A\u4E00\u79CD\u7279\u6B8A\u7684\u6A21\u6001\uFF0C\u5F15\u5165\u6807\u7B7E\u89E3\u6790\u6A21\u5757\u5F97\u5230\u6807\u7B7E\u8BED\u4E49\u8868\u793A\u4EE5\u5173\u8054\u4E0D\u540C\u5A92\u4F53\u6570\u636E\uFF1B \u540C\u65F6\uFF0C\u6784\u5EFA\u7279\u5B9A\u5A92\u4F53\u7684\u8868\u793A\u6A21\u5757\u83B7\u53D6\u5176\u8BED\u4E49\u5171\u4EAB\u8868\u793A\u3002\u5F53\u65B0\u589E\u5A92\u4F53\u7C7B\u578B\u65F6\uFF0C\u4EE5\u5DF2\u7ECF\u5B66\u4E60\u5230\u7684\u6807\u7B7E\u8BED\u4E49\u8868\u793A\u4F5C\u4E3A\u7279\u6743\u8868\u793A\u6765\u5F15\u5BFC\u65B0\u589E\u5A92\u4F53\u6570\u636E\u7684\u8BAD\u7EC3\u3002\u6B64\u5916\uFF0C\u4E00\u4E2A\u7EDF\u4E00\u7684\u5206\u7C7B\u5668\u88AB\u7528\u4E8E\u4E0D\u540C\u5A92\u4F53\u6570\u636E\u7684\u8868\u793A\u6A21\u5757\uFF0C\u4FC3\u8FDB\u4E0D\u540C\u5A92\u4F53\u6570\u636E\u5171\u4EAB\u8868\u793A\u7684\u8BED\u4E49\u5BF9\u9F50\uFF0C\u63D0\u9AD8\u4E86\u8DE8\u5A92\u4F53\u68C0\u7D22\u7684\u6709\u6548\u6027\u548C\u7075\u6D3B\u6027\u3002</p>\",\"dat\":null,\"classify\":\"\",\"id\":12}},\"path\":\"news/info/12\",\"sharedData\":{},\"siteData\":{}}");</script><script defer="" type="text/javascript" src="/templates/styles.0fac0af2.js"></script><script defer="" type="text/javascript" src="/templates/vendors~main.f9fbf9ef.js"></script><script defer="" type="text/javascript" src="/main.a7a8ff5b.js"></script></body></html>